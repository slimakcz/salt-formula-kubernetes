{%- from "kubernetes/map.jinja" import master with context %}
{%- from "kubernetes/map.jinja" import pool with context %}
{%- from "kubernetes/map.jinja" import monitoring with context %}

{%- set network = {} %}
{%- if pool.get('enabled', False) %}
{%- set network = pool.get('network', {}) %}
{%- elif master.get('enabled', False) %}
{%- set network = master.get('network', {}) %}
{%- endif %}

server:
  target:
    kubernetes:
      enabled: true
      api_ip: {{ pool.apiserver.host }}
      cert_name: prometheus-server.crt
      key_name: prometheus-server.key
{%- if network.get('calico', {}).get('enabled', False) and network.calico.get('prometheus', {}).get('enabled', False) %}
    static:
      calico:
        endpoint:
{%- if pool.get('enabled', False) %}
          - address: {{ network.calico.prometheus.get('address', pool.address) }}
{%- else %}
          - address: {{ network.calico.prometheus.get('address', master.address) }}
{%- endif %}
            port: {{ network.calico.prometheus.get('port', 9091) }}
{%- endif %}
  recording:
    cluster_namespace_controller_pod_container:spec_memory_limit_bytes:
      query: >-
        sum by (cluster,namespace,controller,pod_name,container_name) (
          label_replace(
            container_spec_memory_limit_bytes{container_name!=""},
            "controller", "$1",
            "pod_name", "^(.*)-[a-z0-9]+"
          )
        )
    cluster_namespace_controller_pod_container:spec_cpu_shares:
      query: >-
        sum by (cluster,namespace,controller,pod_name,container_name) (
          label_replace(
            container_spec_cpu_shares{container_name!=""},
            "controller", "$1",
            "pod_name", "^(.*)-[a-z0-9]+"
          )
        )
    cluster_namespace_controller_pod_container:cpu_usage:rate:
      query: >-
        sum by (cluster,namespace,controller,pod_name,container_name) (
          label_replace(
            irate(
              container_cpu_usage_seconds_total{container_name!=""}[5m]
            ),
            "controller", "$1",
            "pod_name", "^(.*)-[a-z0-9]+"
          )
        )
    cluster_namespace_controller_pod_container:memory_usage:bytes:
      query: >-
        sum by (cluster,namespace,controller,pod_name,container_name) (
          label_replace(
            container_memory_usage_bytes{container_name!=""},
            "controller", "$1",
            "pod_name", "^(.*)-[a-z0-9]+"
          )
        )
    cluster_namespace_controller_pod_container:memory_working_set:bytes:
      query: >-
        sum by (cluster,namespace,controller,pod_name,container_name) (
          label_replace(
            container_memory_working_set_bytes{container_name!=""},
            "controller", "$1",
            "pod_name", "^(.*)-[a-z0-9]+"
          )
        )
    cluster_namespace_controller_pod_container:memory_rss:bytes:
      query: >-
        sum by (cluster,namespace,controller,pod_name,container_name) (
          label_replace(
            container_memory_rss{container_name!=""},
            "controller", "$1",
            "pod_name", "^(.*)-[a-z0-9]+"
          )
        )
    cluster_namespace_controller_pod_container:memory_cache:bytes:
      query: >-
        sum by (cluster,namespace,controller,pod_name,container_name) (
          label_replace(
            container_memory_cache{container_name!=""},
            "controller", "$1",
            "pod_name", "^(.*)-[a-z0-9]+"
          )
        )
    cluster_namespace_controller_pod_container:disk_usage:bytes:
      query: >-
        sum by (cluster,namespace,controller,pod_name,container_name) (
          label_replace(
            container_fs_usage_bytes{container_name!=""},
            "controller", "$1",
            "pod_name", "^(.*)-[a-z0-9]+"
          )
        )
    cluster_namespace_controller_pod_container:memory_pagefaults:rate:
      query: >-
        sum by (cluster,namespace,controller,pod_name,container_name,scope,type) (
          label_replace(
            irate(
              container_memory_failures_total{container_name!=""}[5m]
            ),
            "controller", "$1",
            "pod_name", "^(.*)-[a-z0-9]+"
          )
        )
    cluster_namespace_controller_pod_container:memory_oom:rate:
      query: >-
        sum by (cluster,namespace,controller,pod_name,container_name,scope,type) (
          label_replace(
            irate(
              container_memory_failcnt{container_name!=""}[5m]
            ),
            "controller", "$1",
            "pod_name", "^(.*)-[a-z0-9]+"
          )
        )
    cluster:memory_allocation:percent:
      query: >-
        100 * sum by (cluster) (
          container_spec_memory_limit_bytes{pod_name!=""}
        ) / sum by (cluster) (
          machine_memory_bytes
        )
    cluster:memory_used:percent:
      query: >-
        100 * sum by (cluster) (
          container_memory_usage_bytes{pod_name!=""}
        ) / sum by (cluster) (
          machine_memory_bytes
        )
    cluster:cpu_allocation:percent:
      query: >-
        100 * sum by (cluster) (
          container_spec_cpu_shares{pod_name!=""}
        ) / sum by (cluster) (
          container_spec_cpu_shares{id="/"} * on(cluster,instance) machine_cpu_cores
        )
  alert:
  {%- set instance_minor_threshold_percent = monitoring.instance_minor_threshold_percent|float %}
  {%- set instance_major_threshold_percent = monitoring.instance_major_threshold_percent|float %}
    ContainerScrapeError:
      if: "container_scrape_error != 0"
    {% raw %}
      labels:
        severity: warning
        service: kubernetes
      annotations:
        summary: "Failed to get the container metrics"
        description: "Prometheus was not able to scrape metrics from the container on the {{ $labels.instance }} instance."
    {% endraw %}
    KubernetesProcessDown:
      if: >-
        procstat_running{process_name=~"hyperkube-.*"} == 0
    {% raw %}
      for: 2m
      labels:
        severity: minor
        service: kubernetes
      annotations:
        summary: "Kubernetes {{ $labels.process_name }} process is down"
        description: "Kubernetes {{ $labels.process_name }} process on the {{ $labels.host }} node is down for at least 2 minutes."
    {% endraw %}
    KubernetesProcessDownMinor:
      if: >-
        count(procstat_running{process_name=~"hyperkube-.*"} == 0) by (process_name) > count(procstat_running{process_name=~"hyperkube-.*"}) by (process_name) * {{ instance_minor_threshold_percent }}
    {% raw %}
      for: 2m
      labels:
        severity: minor
        service: kubernetes
      annotations:
        summary: "{% endraw %}{{ instance_minor_threshold_percent * 100 }}%{% raw %} of Kubernetes {{ $labels.process_name }} process instances are down"
        description: >-
          {{ $value }} of Kubernetes {{ $labels.process_name }} process instances are down {% endraw %}(at least {{ instance_minor_threshold_percent * 100 }}%) for at least 2 minutes.
    KubernetesProcessDownMajor:
      if: >-
        count(procstat_running{process_name=~"hyperkube-.*"} == 0) by (process_name) > count(procstat_running{process_name=~"hyperkube-.*"}) by (process_name) * {{ instance_major_threshold_percent }}
      for: 2m
      labels:
        severity: major
        service: kubernetes
      annotations:
        summary: "{{ instance_major_threshold_percent * 100 }}%{% raw %} of Kubernetes {{ $labels.process_name }} process instances are down"
        description: >-
          {{ $value }} of Kubernetes {{ $labels.process_name }} process instances are down {% endraw %}(at least {{ instance_major_threshold_percent * 100 }}%) for at least 2 minutes.
    KubernetesProcessOutage:
      if: >-
        count(procstat_running{process_name=~"hyperkube-.*"}) by (process_name) == count(procstat_running{process_name=~"hyperkube-.*"} == 0) by (process_name)
    {% raw %}
      for: 2m
      labels:
        severity: critical
        service: kubernetes
      annotations:
        summary: "Kubernetes {{ $labels.process_name }} cluster outage"
        description: "All Kubernetes {{ $labels.process_name }} process instances are down for at least 2 minutes."
    {% endraw %}
{%- if network.get('calico', {}).get('enabled', False) %}
    CalicoProcessDown:
      if: >-
        procstat_running{process_name=~"calico-felix|bird|bird6|confd"} == 0
    {% raw %}
      for: 2m
      labels:
        severity: minor
        service: calico
      annotations:
        summary: "Calico {{ $labels.process_name }} process is down"
        description: "Calico {{ $labels.process_name }} process on the {{ $labels.host }} node is down for at least 2 minutes."
    {% endraw %}
    CalicoProcessDownMinor:
      if: >-
        count(procstat_running{process_name=~"calico-felix|bird|bird6|confd"} == 0) by (process_name) > count(procstat_running{process_name=~"calico-felix|bird|bird6|confd"}) by (process_name) * {{ instance_minor_threshold_percent }}
      for: 2m
      labels:
        severity: minor
        service: calico
      annotations:
        summary: "{{ instance_minor_threshold_percent * 100 }}%{% raw %} of Calico {{ $labels.process_name }} process instances are down"
        description: >-
          {{ $value }} of Calico {{ $labels.process_name }} process instances are down {% endraw %}(at least {{ instance_minor_threshold_percent * 100 }}%) for at least 2 minutes.
    CalicoProcessDownMajor:
      if: >-
        count(procstat_running{process_name=~"calico-felix|bird|bird6|confd"} == 0) by (process_name) > count(procstat_running{process_name=~"calico-felix|bird|bird6|confd"}) by (process_name) * {{ instance_major_threshold_percent }}
      for: 2m
      labels:
        severity: major
        service: calico
      annotations:
        summary: "{{ instance_major_threshold_percent * 100 }}%{% raw %} of Calico {{ $labels.process_name }} process instances are down"
        description: >-
          {{ $value }} of Calico {{ $labels.process_name }} process instances are down {% endraw %}(at least {{ instance_major_threshold_percent * 100 }}%) for at least 2 minutes.
    CalicoProcessOutage:
      if: >-
        count(procstat_running{process_name=~"calico-felix|bird|bird6|confd"}) by (process_name) == count(procstat_running{process_name=~"calico-felix|bird|bird6|confd"} == 0) by (process_name)
    {% raw %}
      for: 2m
      labels:
        severity: critical
        service: calico
      annotations:
        summary: "Calico {{ $labels.process_name }} cluster outage"
        description: "All Calico {{ $labels.process_name }} process instances are down for at least 2 minutes."
    {% endraw %}
{% endif %}
